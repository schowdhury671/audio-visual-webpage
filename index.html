<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Explorations in Audio-Visual Integration and Learning</title>
    <link rel="stylesheet" href="style.css">
    <link rel="icon" href="favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>

    <header>
        <div class="header-container">
            <img src="assets/umd.png" alt="UMD Logo" class="logo-left">
            <h1>Explorations in Audio-Visual Integration and Learning</h1>
            <img src="assets/gamma_logo-2.png" alt="GAMMA Lab Logo" class="logo-right">
        </div>
    </header>

    <!-- Navigation Bar -->
    <nav class="navbar">
        <ul>
            <li><a href="index.html"><i class="fas fa-home"></i> Home</a></li>
            <li><a href="research.html"><i class="fas fa-flask"></i> Research</a></li>
<!--             <li><a href="index.html#people"><i class="fas fa-users"></i> People</a></li> -->
            <li><a href="index.html#updates"><i class="fas fa-bullhorn"></i> Updates</a></li>
            <li><a href="events.html"><i class="fas fa-calendar-alt"></i> Events</a></li>
        </ul>
    </nav>

    <!-- Section 1 -->
    <!-- Introduction Section -->
<section class="intro-section">
    <div class="container">
        <h2>Our Goal</h2>
        <!-- First Paragraph: Full Width -->
        <p class="full-width">
            This research journey begins by establishing the fundamental synergy between sight and sound to solve tangible problems. The work on <b>AdVerb</b>  provides a perfect starting point, demonstrating that visual information—like the geometry of a room—can be a powerful cue to solve a classic audio problem: removing reverberation or echo. Building on this principle of cross-modal assistance, the research then pivots from a corrective task to a creative one with <b>MeLFusion </b>. This paper explores if one can generate an entirely new modality (music) by taking cues from both text and images, proving that audio-visual understanding can be leveraged for complex, generative synthesis.
        </p>
        <!-- Wrapper for Image and Subsequent Paragraphs -->
        <div class="text-image-wrapper">
            <img src="assets/av-teaser.png" alt="AV Teaser Image" class="intro-image">
            <p>
                With these foundational capabilities established, the research takes a significant leap forward by integrating them into the paradigm of Large Language Models. <b>Meerkat</b>  stands as a cornerstone in this narrative, introducing a powerful Audio-Visual LLM (AV-LLM) designed for a deep, fine-grained understanding of the world. By enabling the model to precisely ground sounds in both space and time within a video, Meerkat creates the core engine necessary for all subsequent, more advanced reasoning tasks. It answers not just "what" is in a video, but "where" and "when" the sounds are happening.
            </p>
            <p>
                Once such a powerful engine is built, the focus naturally shifts to maturing it. The research probes its limits and enhances its intelligence through three distinct steps. First, <b>AVTrustBench </b> addresses the critical question of reliability, introducing a benchmark to test the trustworthiness and robustness of these AV-LLMs against adversarial attacks and complex compositional reasoning. Then, <b>AURELIA </b> tackles the challenge of sophisticated reasoning, developing a framework to instill structured, step-by-step thinking into the model at test-time, significantly improving its problem-solving abilities. Finally, <b>MAGNET </b> pushes these reasoning capabilities to their logical extreme, moving from understanding a single video to finding an "audio-visual needle" by reasoning across a "haystack" of multiple videos, a task that mirrors human-like information synthesis.
            </p>
            <p>
                This entire research arc, from foundational fusion to advanced reasoning, culminates in addressing the ultimate challenge: real-world deployment. <b>EGOADAPT </b> brings the story full circle by focusing on efficiency. It presents an adaptive framework designed for egocentric perception (like on AR glasses), where the model intelligently learns which sensors to use at any given moment to save power and computational resources. This final step shows a clear path from building powerful, large-scale models in a lab to deploying smart, efficient, and trustworthy AI that can understand and interact with the complex, multi-modal world around us.
            </p>
        </div>
    </div>

                    <!-- <div class="team-member">
                        <a href="https://utkarsh4430.github.io/">
                            <img src="people/utkarsh.jpeg" alt="Person's Name2 Headshot" class="team-headshot">
                            <p class="team-name">Utkarsh Tyagi</p>
                        </a>
                    </div> -->
                    
                      
                </div>
            </div>
        </div>
    </section>
</section>

    <!-- Announcement Section -->
    <section class="updates-section" id="updates">
        <div class="container">
            <h2>Updates!</h2>
            <div class="updates-content">
                <p><strong>June 2025:</strong> 3 papers accepted at ICCV 2025!</p>
                <p><strong>June 2025:</strong> Co-organising  <a href="https://gen4avc.github.io/"> <font color="#ff0000">Gen4AVC workshop</font> </a>  to be held in conjunction with ICCV 2025.!</p>
                <p><strong>July 2024:</strong> Work on Audio-Visual LLM got accepted to <a href="https://eccv.ecva.net/"> <font color="#ff0000"> ECCV 2024 </font></a> !</p>
                <p><strong>Feb 2024:</strong> Work on audio-visual music generation got accepted as a highlight at CVPR 2024. </p>
                <p><strong>July 2023:</strong> 1 paper got accepted at ICCV 2023! </p>
            </div>
        </div>
    </section>


    <!-- Repeat the above project section for each project, updating content and links accordingly -->

    <!-- Footer -->
    <footer>
        <p>&copy; 2025 UMD GAMMA Lab</p>
    </footer>

    <!-- Optional JavaScript for Interactive Effects -->
    <script src="script.js"></script>
</body>
</html>
