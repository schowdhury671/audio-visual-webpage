
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Research - Audio-Visual Processing</title>
    <link rel="stylesheet" href="style.css">
    <link rel="icon" href="favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>

    <header>
        <div class="header-container">
            <img src="assets/umd.png" alt="UMD Logo" class="logo-left">
            <h1>Research at GAMMA Lab</h1>
            <img src="assets/gamma_logo-2.png" alt="GAMMA Lab Logo" class="logo-right">
        </div>
    </header>

    <!-- Navigation Bar -->
    <nav class="navbar">
        <ul>
            <li><a href="index.html"><i class="fas fa-home"></i> Home</a></li>
            <li><a href="research.html"><i class="fas fa-flask"></i> Research</a></li>
	    <li><a href="dataset.html"><i class="fas fa-flask"></i> Datasets</a></li>
            <li><a href="index.html#people"><i class="fas fa-users"></i> People</a></li>
            <li><a href="index.html#updates"><i class="fas fa-bullhorn"></i> Updates</a></li>
            <li><a href="events.html"><i class="fas fa-calendar-alt"></i> Events</a></li>
        </ul>
    </nav>

    <!-- Research Publications Section -->
    <section class="container">
            <!-- Projects Sections -->
    <section class="intro-section">
        <div class="container">
            <h2>Publications</h2>
        </div>
    </section>
    <section class="project-section">
        <div class="container">
            <div class="text-content">
                <h2>MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks<br> <span style="color:rgb(0, 26, 255)">Under Review</span></h2>
                <p>
                    We present AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA pairs designed to assess the capabilities of LMMs in multi-video retrieval and temporal grounding task.
			The benchmark curation pipeline consists of five stages (1) Video curation  (2) Blind question generation  (3) Transcript cleaning and segmentation  (4) Segment-aware QA prompting
			(5) Answer grounding. Each QA item includes: (i) a free-form question, (ii) a step-by-step answer, and (iii) a list of ⟨videoID, start, end⟩ references. Unlike prior single-clip 			datasets, 82% of our QA pairs require evidence from at least two distinct videos, making them well-suited for LMMs.
			
			
                </p>
                <div class="buttons">
                    <a href="https://schowdhury671.github.io/magnet_project/" target="_blank" class="btn">Dataset</a>
                    
                </div>
            </div>
            <div class="image-content">
                <img src="assets/magnet.png" alt="magnet Project Image">
            </div>
        </div>
    </section>
        
    <section class="project-section">
        <div class="container">
            <div class="text-content">
                <h2>AURELIA: Test-time Reasoning Distillation in Audio-Visual LLMs <br> <span style="color:rgb(0, 26, 255)">ICCV 2025</span></h2>
                </h2>
                <p>
                    We present AVReasonBench, a challenging benchmark comprising 4500 audio-visual questions, each paired with detailed step-by-step reasoning. Our benchmark spans six distinct tasks, 		    including AV-GeoIQ, which evaluates AV reasoning combined with geographical and cultural knowledge. We carefully curate 1000 samples each from Music-AVQA,
		AVSD, and VALOR which are suitable for AV reasoning. For
		the AV compositional understanding task, we collect 1000
		samples from the web through careful manual inspection.
		For AV-GeoIQ we again tailor-make 200 samples which require strong AV reasoning capabilities. We augment more
		videos to the original AV-meme set to make a total of 100 test
		samples while we adapt 200 samples of DM-Match to make
		the total size of our reasoning benchmark, AVReasonBench
		to 4500.
                </p>
                <div class="buttons">
                    <a href="https://schowdhury671.github.io/aurelia_project/" target="_blank" class="btn">Dataset</a>
                    
                </div>
            </div>
            <div class="image-content">
                <img src="assets/aurelia.png" alt="Aurelia Project Image">
            </div>
        </div>
    </section>

    <section class="project-section">
        <div class="container">
            <div class="text-content">
                <h2>AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs <br> <span style="color:rgb(0, 26, 255)">ICCV 2025</span></h2>
                </h2>
                <p>
                   We we introduce Audio-Visual Trustworthiness assessment Benchmark - AVTrustBench, comprising 600K samples spanning over 9 meticulously crafted tasks, evaluating the capabilities
		of AVLLMs across three distinct dimensions: Adversarial attack, Compositional reasoning, and Modality-specific dependency. Our goal is to investigate the degree to which AVLLMs: accurately 		comprehend the audio, visual, and textual inputs with correct semantics, rely on individual modalities, and follow instructions, even in the presence of inconsistencies in input signals. 
                </p>
                <div class="buttons">
                    <a href="https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FAVTrustBench&ga=1" target="_blank" class="btn">Dataset</a>
                    
                </div>
            </div>
            <div class="image-content">
                <img src="assets/avtrustbench.png" alt="AVTrustBench Project Image">
            </div>
        </div>
    </section>



    <section class="project-section">
        <div class="container">
            <div class="text-content">
                <h2>Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time <br> <span style="color:rgb(0, 26, 255)">ECCV 2024</span></h2>
                </h2>
                <p>
                    We present MeerkatBench: A Unified Benchmark Suite for
			Fine-grained Audio-Visual Understanding by introducing the following tasks: (i) audio referred image grounding, (ii) image guided audio temporal localization, (iii) audio-visual 			fact-checking, and two coarse-grained tasks: (iv) audio-visual question answering, (v) audio-visual captioning. To this end we curate a large dataset AVFIT that comprises 3M 				instruction tuning samples collected from open-source datasets. AVFIT consists of samples collected in the following ways: (i) suitable adaptation of public datasets and (ii)
instruction-tuning data generation via prompting GPT-3.5    
                </p>
                <div class="buttons">
                    <a href="https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FAVFIT%20dataset&ga=1" target="_blank" class="btn">Dataset</a>
                    
                </div>
            </div>
            <div class="image-content">
                <img src="assets/meerkat.png" alt="Meerkat Project Image">
            </div>
        </div>
    </section>


    <section class="project-section">
        <div class="container">
            <div class="text-content">
                <h2>MeLFusion: Synthesizing Music from Image and Language Cues using Diffusion Models<br> <span style="color:rgb(0, 26, 255)">CVPR 2024 (Highlight)</span></h2>
                </h2>
                <p>
                  We collect a new dataset MeLBench, which contains 11,250 manually annotated triplets of ⟨Image, Text, Music⟩. Further, we extend the MusicCaps dataset which contains ⟨Text, Music⟩ pairs      		 by adding the corresponding image.  For each videos in the dataset, the annotators were asked to provide (a) a free-form text description for up to three sentences, expressing the 			composition and (b) any other music-related details such as describing the genre, mood, tempo, singer voices, instrumentation, dissonances, rhythm, etc. A carefully selected frame and music 		from the snippet along with text description from annotators forms ⟨Image, Text, Music⟩ triplets. We also extend MusicCaps by carefully choosing two images from the web or YouTube that can 		go well with each datapoint in MusicCaps, thereby extending ⟨Text, Music⟩ pairs to ⟨Image, Text, Music⟩ triplets. 
                </p>
                <div class="buttons">
                    <a href="https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FMeLFusion%20datasets&ga=1" target="_blank" class="btn">Dataset</a>
                    
                </div>
            </div>
            <div class="image-content">
                <img src="assets/melfusion.png" alt="Melfusion Project Image">
            </div>
        </div>
    </section>


    
    <!-- Footer -->
    <footer>
        <p>&copy; 2025 UMD GAMMA</p>
    </footer>

</body>
</html>
