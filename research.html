<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Research - Audio-Visual Processing</title>
    <link rel="stylesheet" href="style.css">
    <link rel="icon" href="favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>

    <header>
        <div class="header-container">
            <img src="assets/umd.png" alt="UMD Logo" class="logo-left">
            <h1>Research at GAMMA Lab</h1>
            <img src="assets/gamma_logo-2.png" alt="GAMMA Lab Logo" class="logo-right">
        </div>
    </header>

    <!-- Navigation Bar -->
    <nav class="navbar">
        <ul>
            <li><a href="index.html"><i class="fas fa-home"></i> Home</a></li>
            <li><a href="research.html"><i class="fas fa-flask"></i> Research</a></li>
<!--             <li><a href="index.html#people"><i class="fas fa-users"></i> People</a></li> -->
            <li><a href="index.html#updates"><i class="fas fa-bullhorn"></i> Updates</a></li>
            <li><a href="events.html"><i class="fas fa-calendar-alt"></i> Events</a></li>
        </ul>
    </nav>

    <!-- Research Publications Section -->
    <section class="container">
            <!-- Projects Sections -->
    <section class="intro-section">
        <div class="container">
            <h2>Publications</h2>
        </div>
    </section>
    <section class="project-section">
        <div class="container">
            <div class="text-content">
                <h2>MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks<br> <span style="color:rgb(0, 26, 255)">Under Review</span></h2>
                <p>
                    Large multimodal models (LMMs) have shown remarkable progress in audio-visual understanding, yet they struggle with real-world scenarios that require complex reasoning across extensive video collections. Existing benchmarks for video question answering remain limited in scope, typically involving one clip per query, which falls short of representing the challenges of large-scale, audio-visual retrieval and reasoning encountered in practical applications. To bridge this gap, we introduce a novel task named AV-HaystacksQA, where the goal is to identify salient segments across different videos in response to a query and link them together to generate the most informative answer. To this end, we present AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA pairs designed to assess the capabilities of LMMs in multi-video retrieval and temporal grounding task. Additionally, we propose a model-agnostic, multi-agent framework MAGNET to address this challenge, achieving up to 89% and 65% relative improvements over baseline methods on BLEU@4 and GPT evaluation scores in QA task on our proposed AVHaystacks. To enable robust evaluation of multi-video retrieval and temporal grounding for optimal response generation, we introduce two new metrics, STEM, which captures alignment errors between a ground truth and a predicted step sequence and MTGS, to facilitate balanced and interpretable evaluation of segment-level grounding performance.
                </p>
                <div class="buttons">
                    <a href="https://www.arxiv.org/pdf/2506.07016" target="_blank" class="btn">arXiv</a>
                    <a href="https://schowdhury671.github.io/magnet_project/" target="_blank" class="btn">Project Page</a>
                    <!-- <a href="" target="_blank" class="btn">Code</a> -->
                </div>
            </div>
            <div class="image-content">
                <img src="assets/magnet.png" alt="magnet Project Image">
            </div>
        </div>
    </section>
        
<!--     <section class="project-section">
        <div class="container">
            <div class="text-content">
                <h2>SILA: Signal-to-Language Augmentation for Enhanced Control in Text-to-Audio Generation <br> <span style="color:rgb(0, 26, 255)">Under Review</span></h2>
                </h2>
                <p>The field of text-to-audio generation has seen significant advancements, and yet the ability to finely control the acoustic characteristics of generated audio remains under-explored. In this paper, we introduce a novel yet simple approach to generate sound effects with control over key acoustic parameters such as loudness, pitch, reverb, fade, brightness, noise and duration, enabling creative applications in sound design and content creation. These parameters extend beyond traditional Digital Signal Processing (DSP) techniques, incorporating learned representations that capture the subtleties of how sound characteristics can be shaped in context, enabling a richer and more nuanced control over the generated audio. Our approach is model-agnostic and is based on learning the disentanglement between audio semantics and its acoustic features. Our approach not only enhances the versatility and expressiveness of text-to-audio generation but also opens new avenues for creative audio production and sound design. Our objective and subjective evaluation results demonstrate the effectiveness of our approach in producing high-quality, customizable audio outputs that align closely with user specifications.</p>
                <div class="buttons">
                    <a href="https://arxiv.org/abs/2412.09789" target="_blank" class="btn">arXiv</a>
                </div>
            </div>
            <div class="image-content">
                <img src="assets/sila_web.png" alt="SILA Project Image">
            </div>
        </div>
    </section> -->




    
    <!-- Footer -->
    <footer>
        <p>&copy; 2025 UMD GAMMA</p>
    </footer>

</body>
</html>
